import os
import smtplib
import feedparser
import urllib.parse
import time
import re
from datetime import datetime, timedelta
from time import mktime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from deep_translator import GoogleTranslator

# --------------------------------------------------------------------------------
# 1. æ ¸å¿ƒé…ç½®ä¸å·¥å…·å‡½æ•°
# --------------------------------------------------------------------------------

def get_sim_hash(title):
    """æå–æ ‡é¢˜ç‰¹å¾æŒ‡çº¹ç”¨äºå»é‡"""
    clean = "".join(re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]', title))
    return clean[:25].lower()

def is_garbage_news(title):
    """è¿‡æ»¤äººäº‹å˜åŠ¨ã€å§”ä»»ç­‰éä¸šåŠ¡èµ„è®¯"""
    garbage_keywords = [
        'board member', 'appoints', 'appointment', 'resigns', 'joins', 
        'promotion', 'hiring', 'CEO', 'CFO', 'VP', 'äººäº‹', 'ä»»èŒ', 'è‘£äº‹ä¼š', 'å§”ä»»'
    ]
    title_lower = title.lower()
    return any(k in title_lower for k in garbage_keywords)

def fetch_edu_news(days=30):
    translator = GoogleTranslator(source='auto', target='zh-CN')
    threshold = datetime.now() - timedelta(days=days)
    results = {"china": [], "intl": []}
    seen_fingerprints = set()

    # --- A. 20ä¸ªç‰¹å®šçš„æµ·å¤–åæ ¡å®˜æ–¹ RSS é¢‘é“ ---
    specific_uni_feeds = [
        "https://news.harvard.edu/gazette/feed/",
        "https://news.stanford.edu/feed/",
        "https://www.ox.ac.uk/news-rss-feed",
        "https://www.cam.ac.uk/news/feed",
        "https://web.mit.edu/news/rss/topic/education.xml",
        "https://news.yale.edu/topics/education/rss",
        "https://www.princeton.edu/news/rss",
        "https://www.upenn.edu/penn-news/rss",
        "https://www.cornell.edu/news/rss",
        "https://www.ucl.ac.uk/news/rss",
        "https://www.imperial.ac.uk/news/rss",
        "https://www.lse.ac.uk/News/RSS-Feeds",
        "https://news.berkeley.edu/feed/",
        "https://news.uchicago.edu/rss-feeds",
        "https://www.unimelb.edu.au/news/rss",
        "https://www.sydney.edu.au/news-opinion/rss.xml",
        "https://www.nyu.edu/about/news-publications/news/rss.xml",
        "https://www.nus.edu.sg/news/rss",
        "https://www.utoronto.ca/news/feed",
        "https://www.ethz.ch/en/news-and-events/eth-news/rss.xml"
    ]

    # --- B. ä¸­å›½æ•™è‚²åŠ¨æ€æŸ¥è¯¢ (äº¬æ²ªæ­æ·±/C9/AIå®è·µ) ---
    china_queries = [
        '(åŒ—äº¬ OR ä¸Šæµ· OR æ·±åœ³ OR æ­å·) (å›½é™…å­¦æ ¡ OR åæ ¡) (å½•å– OR æ‹›ç”Ÿ OR å‡å­¦æ¦œå• OR æ”¹é©)',
        '(æ–°æµªæ•™è‚² OR é¡¶æ€ OR è…¾è®¯æ•™è‚²) (AIå®è·µ OR æ™ºæ…§æ•™è‚² OR æ•™è‚²æ•°å­—åŒ– OR æ•™æˆè§‚ç‚¹)',
        '("C9é«˜æ ¡" OR æ¸…å OR åŒ—å¤§ OR å¤æ—¦ OR æµ™å¤§) (é’ˆå¯¹ä¸­å›½å­¦ç”Ÿ OR æ‹›ç”Ÿç®€ç«  OR æ¥åç•™å­¦)'
    ]

    # --- C. å›½é™…è§†é‡å¹¿åŸŸæŸ¥è¯¢ (è¡¥å……æº) ---
    intl_queries = [
        '("Top 100 Universities" OR "Ivy League") (Admissions for Chinese students OR Visa OR Requirements)',
        '("EdSurge" OR "EdWeek") (AI classroom practice OR Generative AI Case Study OR Implementation)',
        '(Professor OR Scholar OR Dean) (Future of Education OR AI Trends OR Insight)'
    ]

    # 1. æŠ“å–ä¸­å›½åŒºåŠ¨æ€ (é™é¢ 10 æ¡)
    for q in china_queries:
        url = f"https://news.google.com/rss/search?q={urllib.parse.quote(q)}&hl=zh-CN&gl=CN&ceid=CN:zh-Hans"
        feed = feedparser.parse(url)
        for entry in feed.entries:
            if not hasattr(entry, 'published_parsed'): continue
            pub_time = datetime.fromtimestamp(mktime(entry.published_parsed))
            if pub_time < threshold or is_garbage_news(entry.title): continue
            
            fp = get_sim_hash(entry.title)
            if fp not in seen_fingerprints and len(results["china"]) < 10:
                seen_fingerprints.add(fp)
                results["china"].append({
                    "title": entry.title,
                    "eng_title": "",
                    "source": entry.source.get('title', 'ä¸­å›½æ•™è‚²æº'),
                    "url": entry.link,
                    "date": pub_time.strftime('%m-%d')
                })
        time.sleep(0.5)

    # 2. æŠ“å–ç‰¹å®šåæ ¡ RSS æº (é™é¢ 10 æ¡ä¼˜å…ˆå¡«å……)
    for feed_url in specific_uni_feeds:
        if len(results["intl"]) >= 10: break
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries:
                pub_time = datetime.fromtimestamp(mktime(entry.published_parsed)) if hasattr(entry, 'published_parsed') else datetime.now()
                if pub_time < threshold or is_garbage_news(entry.title): continue
                
                fp = get_sim_hash(entry.title)
                if fp not in seen_fingerprints and len(results["intl"]) < 10:
                    seen_fingerprints.add(fp)
                    try: chi_title = translator.translate(entry.title)
                    except: chi_title = entry.title
                    results["intl"].append({
                        "title": chi_title,
                        "eng_title": entry.title,
                        "source": "åæ ¡å®˜æ–¹é¢‘é“",
                        "url": entry.link,
                        "date": pub_time.strftime('%m-%d')
                    })
        except: continue

    # 3. æŠ“å–å›½é™…å¹¿åŸŸæº (è‹¥ RSS æœªæ»¡ 10 æ¡åˆ™è¡¥è¶³)
    if len(results["intl"]) < 10:
        for q in intl_queries:
            url = f"https://news.google.com/rss/search?q={urllib.parse.quote(q)}&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if not hasattr(entry, 'published_parsed'): continue
                pub_time = datetime.fromtimestamp(mktime(entry.published_parsed))
                if pub_time < threshold or is_garbage_news(entry.title): continue
                
                fp = get_sim_hash(entry.title)
                if fp not in seen_fingerprints and len(results["intl"]) < 10:
                    seen_fingerprints.add(fp)
                    try: chi_title = translator.translate(entry.title)
                    except: chi_title = entry.title
                    results["intl"].append({
                        "title": chi_title,
                        "eng_title": entry.title,
                        "source": entry.source.get('title', 'å…¨çƒæ•™è‚²è§†é‡'),
                        "url": entry.link,
                        "date": pub_time.strftime('%m-%d')
                    })
            time.sleep(1)

    return results

# --------------------------------------------------------------------------------
# 2. é‚®ä»¶æ ¼å¼åŒ–ä¸å‘é€
# --------------------------------------------------------------------------------

def format_html(data):
    sections = [
        ("china", "ğŸ‡¨ğŸ‡³ ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸­å›½æ•™è‚²æ´å¯Ÿ (äº¬æ²ªæ­æ·±/C9/åæ ¡)", "#c02424"),
        ("intl", "ğŸŒ ç¬¬äºŒéƒ¨åˆ†ï¼šå›½å¤–æ•™è‚²æ´å¯Ÿ (TOP100åæ ¡/AIå®è·µ/ä¸“å®¶è§‚ç‚¹)", "#1a365d")
    ]
    
    rows = ""
    for key, name, color in sections:
        rows += f'<tr><td style="padding:15px; background:{color}; color:#fff; font-size:16px; font-weight:bold;">{name}</td></tr>'
        if not data[key]:
            rows += '<tr><td style="padding:20px; text-align:center; color:#94a3b8; background:#fff;">æœ¬æœŸæš‚æ— åŒ¹é…çš„é«˜ä»·å€¼æ·±åº¦åŠ¨æ€</td></tr>'
        else:
            for i, item in enumerate(data[key], 1):
                eng_html = f'<div style="font-size:11px; color:#64748b; margin-top:4px;">{item["eng_title"]}</div>' if item["eng_title"] else ""
                rows += f"""
                <tr><td style="padding:15px; border-bottom:1px solid #e2e8f0; background:#fff;">
                    <div style="font-size:14px; font-weight:bold; color:#1e293b; line-height:1.4;">{i:02d} {item['title']}</div>
                    {eng_html}
                    <div style="font-size:11px; color:#94a3b8; margin-top:8px;">
                        <span>ğŸ¢ {item['source']}</span> | <span>ğŸ“… {item['date']}</span> | 
                        <a href="{item['url']}" style="color:{color}; text-decoration:none; font-weight:bold;">æŸ¥çœ‹åŸæ–‡ â†’</a>
                    </div>
                </td></tr>
                """
    return rows

def send_email():
    sender = "alexanderxyh@gmail.com"
    pw = os.environ.get('EMAIL_PASSWORD')
    receivers = ["47697205@qq.com", "54517745@qq.com"]
    
    news_data = fetch_edu_news(days=30)
    content_html = format_html(news_data)
    
    email_template = f"""
    <html><body style="font-family:'PingFang SC',Arial,sans-serif; background:#f1f5f9; padding:15px;">
        <div style="max-width:700px; margin:0 auto; background:#fff; border-radius:8px; overflow:hidden; border:1px solid #e2e8f0; box-shadow:0 4px 6px rgba(0,0,0,0.05);">
            <div style="background:#1e293b; padding:30px; text-align:center; color:#fff;">
                <h1 style="margin:0; font-size:22px;">Alex's Education Intelligence</h1>
                <p style="font-size:13px; opacity:0.8; margin-top:8px;">30å¤©å…¨çƒæ·±åº¦æ´å¯Ÿï¼šä¸­å›½åæ ¡ã€æµ·å¤–åæ ¡ã€AIæ•™è‚²æ¡ˆä¾‹</p>
            </div>
            <table style="width:100%; border-collapse:collapse;">{content_html}</table>
            <div style="padding:20px; text-align:center; font-size:11px; color:#94a3b8; background:#f8fafc;">
                å»é‡æœºåˆ¶å·²å¼€å¯ | æœç´¢è·¨åº¦ï¼š30å¤© | ä¿¡å·æºï¼šTop 50 ä¸­å›½æº & 20æ‰€å…¨çƒåæ ¡å®˜æ–¹RSS
            </div>
        </div>
    </body></html>
    """

    msg = MIMEMultipart()
    # æŒ‰ç…§è¦æ±‚ä¿®æ”¹çš„ä¸»é¢˜
    msg['Subject'] = f"Yingå¤§äººçš„'å‚ç›´æ•™è‚²æƒ…æŠ¥æ¯æ—¥æ»šåŠ¨åˆ·æ–°'ï¼š30å¤©å…¨çƒæ·±åº¦ç²¾åç‰ˆ (10+10) ({datetime.now().strftime('%m/%d')})"
    msg['From'] = f"Edu Intelligence Agent <{sender}>"
    msg['To'] = ", ".join(receivers)
    msg.attach(MIMEText(email_template, 'html'))

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(sender, pw)
            server.send_message(msg)
        print(f"âœ… æˆåŠŸå‘é€æ·±åº¦æ´å¯ŸæŠ¥å‘Šã€‚")
    except Exception as e:
        print(f"âŒ å‘é€å¤±è´¥: {e}")

if __name__ == "__main__":
    send_email()
